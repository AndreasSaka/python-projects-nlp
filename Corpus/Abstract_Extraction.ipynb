{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Abstract_Extraction.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GawdTbYJ6D7a"
      },
      "source": [
        "**Installing Biopython**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KfgvZDVn5_xh"
      },
      "source": [
        "!pip install biopython\n",
        "!pip install --upgrade biopython\n",
        "!pip install metapub"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9khRMT9G5nKO"
      },
      "source": [
        "import Bio\n",
        "import metapub\n",
        "from metapub import PubMedFetcher\n",
        "from Bio import Entrez\n",
        "from Bio import SeqIO\n",
        "from Bio.Entrez import efetch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RxzXb2NN3m5_"
      },
      "source": [
        "import pandas as pd # Load the Pandas libraries with alias 'pd' "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajr0L59d8AhV"
      },
      "source": [
        "# (in the same directory that the python process is based)\n",
        "#pmid = pd.read_csv(\"/content/only pubmid 799 from ewag.csv\",header=None)  # Control delimiters, rows, column names with read_csv (see later) \n",
        "#pmid = pd.read_csv(\"/content/combo 190 799.csv\", header=None) # Read data from file 'filename.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rt--25Qt3gdR"
      },
      "source": [
        "pmid = pd.read_csv(\"/content/826.txt\",header=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73lJn0QOb3pJ"
      },
      "source": [
        "pmid = pmid.drop(pmid.columns[1], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sONRUkhW8Ae5"
      },
      "source": [
        "pmid.head() # Preview the first 5 lines of the loaded data "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "koEZ__0E8Acz"
      },
      "source": [
        "pmid.columns = ['Pudmid']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xy4kFVHn8AaR"
      },
      "source": [
        "pmid_new = pmid[\"Pudmid\"].tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8B1qHYZO8AWY"
      },
      "source": [
        "len(pmid_new)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-0itC-oNiBq"
      },
      "source": [
        "#pmid_new = \"PMID31563105,PMID31549657,PMID31445334,PMID31430718\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "blTQr1feYL_K"
      },
      "source": [
        "#pmid_new = \"31563105,31549657,31445334,31430718,31398235,31394342,31351302,31026607,30986386\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ZgoWYGIRymm"
      },
      "source": [
        "pmid_new = \"2604392,2604394,2604396,2624462,2646661,2729985\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtV5jWf8UcBt"
      },
      "source": [
        "Entrez.email = \"a.sakapetis@students.uu.nl\"\n",
        "handle = efetch(db='pubmed', id=pmid_new, retmode='text', rettype='abstract')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRTqga_edGpj"
      },
      "source": [
        "#Entrez.email = \"a.sakapetis@students.uu.nl\"\n",
        "#handle = efetch(db='pubmed', id=pmid_new, retmode='xml', rettype='text')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GiOL7U2P9Ogy"
      },
      "source": [
        "text_from_file = handle.read()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdkZYmIwGYzp"
      },
      "source": [
        "file = open(\"190_799.txt\", \"w\") \n",
        "file.write(str(text_from_file)) \n",
        "file.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vbwx7udjs_jD"
      },
      "source": [
        "with open(\"/content/clean.txt\", 'rb') as corpus:\n",
        "  text_full = corpus.read()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWjglMh-wyl5"
      },
      "source": [
        "import nltk, re, string, unicodedata, inflect, glob, os\n",
        "import re\n",
        "from nltk.corpus.reader.plaintext import PlaintextCorpusReader\n",
        "import numpy as np \n",
        "import pandas as pd                                 #for data manipulation and analysis\n",
        "from nltk.corpus import stopwords                   #Stopwords corpus\n",
        "from nltk.stem import PorterStemmer \n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from string import punctuation\n",
        "from nltk import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNC9OeHaw8lT"
      },
      "source": [
        "stop_words = stopwords.words('english')\n",
        "stop_words.extend(['from','at','subject','for','of','and','re','edu','use'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWV-4DHo00CK"
      },
      "source": [
        "**1) FIRST STEP I WILL REMOVE NOISE**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwfQhh_cUiw8"
      },
      "source": [
        "erst = sent_tokenize(str(text_from_file))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gf69l0NCYc20"
      },
      "source": [
        "erst"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yshuRLlhNsSv"
      },
      "source": [
        "value = text_from_file.lower()\n",
        "value = re.sub(r'[\\r\\n]+', ' ', value)\n",
        "value = re.sub(r'[^\\x00-\\x7F]+', ' ', value)\n",
        "\n",
        "tokenized = TreebankWordTokenizer().tokenize(value)\n",
        "sentence = ' '.join(tokenized)\n",
        "sentence = re.sub(r\"\\s's\\b\", \"'s\", sentence)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vG-SO0rHOSFJ"
      },
      "source": [
        "seconde = sent_tokenize(str(text_from_file))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eEc24yiDq9F8"
      },
      "source": [
        "df = pd.DataFrame(data = seconde)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5mZhC4s9rUCN"
      },
      "source": [
        "df.columns = [\"Sent\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OEcow7QysAA4"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2vqZY89rsyW"
      },
      "source": [
        "df['Sent'] = df['Sent'].str.lower()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VzEi15oxvEKn"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fMDLUGN6gim3"
      },
      "source": [
        "df[\"text_lower\"] = df[\"Sent\"].str.lower()\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SrO27l5Egsj9"
      },
      "source": [
        "# drop the new column created in last cell\n",
        "df.drop([\"Sent\"], axis=1, inplace=True)\n",
        "\n",
        "PUNCT_TO_REMOVE = string.punctuation\n",
        "def remove_punctuation(text):\n",
        "    \"\"\"custom function to remove the punctuation\"\"\"\n",
        "    return text.translate(str.maketrans('', '', PUNCT_TO_REMOVE))\n",
        "\n",
        "df[\"text_wo_punct\"] = df[\"text_lower\"].apply(lambda text: remove_punctuation(text))\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xrsXs0LMgtKP"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "\", \".join(stopwords.words('english'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t9wVBCCuhBrX"
      },
      "source": [
        "STOPWORDS = set(stopwords.words('english'))\n",
        "def remove_stopwords(text):\n",
        "    \"\"\"custom function to remove the stopwords\"\"\"\n",
        "    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n",
        "\n",
        "df[\"text_wo_stop\"] = df[\"text_wo_punct\"].apply(lambda text: remove_stopwords(text))\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HIUwgoXrhDmE"
      },
      "source": [
        "from collections import Counter\n",
        "cnt = Counter()\n",
        "for text in df[\"text_wo_stop\"].values:\n",
        "    for word in text.split():\n",
        "        cnt[word] += 1\n",
        "        \n",
        "cnt.most_common(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgimykEuhHOP"
      },
      "source": [
        "FREQWORDS = set([w for (w, wc) in cnt.most_common(10)])\n",
        "def remove_freqwords(text):\n",
        "    \"\"\"custom function to remove the frequent words\"\"\"\n",
        "    return \" \".join([word for word in str(text).split() if word not in FREQWORDS])\n",
        "\n",
        "df[\"text_wo_stopfreq\"] = df[\"text_wo_stop\"].apply(lambda text: remove_freqwords(text))\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FzShMzLohH5H"
      },
      "source": [
        "# Drop the two columns which are no more needed \n",
        "df.drop([\"text_wo_punct\", \"text_wo_stop\"], axis=1, inplace=True)\n",
        "\n",
        "n_rare_words = 10\n",
        "RAREWORDS = set([w for (w, wc) in cnt.most_common()[:-n_rare_words-1:-1]])\n",
        "def remove_rarewords(text):\n",
        "    \"\"\"custom function to remove the rare words\"\"\"\n",
        "    return \" \".join([word for word in str(text).split() if word not in RAREWORDS])\n",
        "\n",
        "df[\"text_wo_stopfreqrare\"] = df[\"text_wo_stopfreq\"].apply(lambda text: remove_rarewords(text))\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8GrqJy3nhKCS"
      },
      "source": [
        "from collections import Counter\n",
        "cnt = Counter()\n",
        "for text in df[\"text_wo_stopfreqrare\"].values:\n",
        "    for word in text.split():\n",
        "        cnt[word] += 1\n",
        "        \n",
        "cnt.most_common(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_dfXasMdRej9"
      },
      "source": [
        "#Bert"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RSZcgdyQS4JC"
      },
      "source": [
        "with open('output.txt', 'w') as f:\n",
        "    for text in df['text_wo_punct'].tolist():\n",
        "        f.write(text + '\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7S-TvCHyoKE"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "lines = []\n",
        "with open(\"/content/output.txt\", 'r') as infile:\n",
        "    stripped = [line.strip() for line in infile]\n",
        "    for line in stripped:\n",
        "        line = \"\".join(line.strip())\n",
        "        lines.append(line)\n",
        "\n",
        "zeros = [0 for i in range(len(lines))]\n",
        "\n",
        "df = pd.DataFrame(list(zip(lines, zeros)),\n",
        "                  columns=['sentence', 'label'])\n",
        "df.index.name = \"index\"\n",
        "\n",
        "with open('sentence.tsv', 'w') as write_tsv:\n",
        "    write_tsv.write(df.to_csv(sep='\\t', index=True))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4lDrjnMJTKPo"
      },
      "source": [
        "**Tokenize Words for SciSpacy Implementation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQkP5fLgTNmr"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RNP8Jv1iTamy"
      },
      "source": [
        "text_for_spacy = word_tokenize(str(text))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JT9M2xSmUOCk"
      },
      "source": [
        "table = str.maketrans('', '', string.punctuation)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SkgIrtYnUbcG"
      },
      "source": [
        "text_for_spacy_stripped = [w.translate(table) for w in text_for_spacy]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMEZbYX-UTs8"
      },
      "source": [
        "text_for_spacy_stripped"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6GTbt2wC4PL6"
      },
      "source": [
        "type(text_for_spacy_stripped)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1PGJhAo4SJu"
      },
      "source": [
        "file = open(\"erst.txt\", \"w\") \n",
        "file.write(str(erst)) \n",
        "file.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WiZrGX0y28U8"
      },
      "source": [
        "**START SCISPACY IMPLEMENTATION**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0SuWn1K43Bbn"
      },
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "import pickle\n",
        "import spacy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q26bLTaf3Dhd"
      },
      "source": [
        "pip install scispacy spacy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQRmzaFY3WU5"
      },
      "source": [
        "pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.3/en_ner_bc5cdr_md-0.2.3.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yozRO1hSdPcG"
      },
      "source": [
        "pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.3/en_ner_craft_md-0.2.3.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IP8WsAAedScS"
      },
      "source": [
        "pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.3/en_ner_jnlpba_md-0.2.3.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KhZuTAvJdVoQ"
      },
      "source": [
        "pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.3/en_ner_bionlp13cg_md-0.2.3.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXrsmBa23Ymk"
      },
      "source": [
        "import scispacy\n",
        "from spacy import displacy\n",
        "from scispacy.abbreviation import AbbreviationDetector\n",
        "from scispacy.umls_linking import UmlsEntityLinker"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5qgrBHlJderp"
      },
      "source": [
        "import en_ner_craft_md\n",
        "import en_ner_jnlpba_md\n",
        "import en_ner_bc5cdr_md"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVH2nggM3Zx-"
      },
      "source": [
        "import en_ner_bionlp13cg_md\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lc5ra4sY3cSB"
      },
      "source": [
        "nlp = en_ner_bc5cdr_md.load()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vt6pn8L-3e2j"
      },
      "source": [
        "#nlp.max_length = 4421830             "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BeTJSqfyQcso"
      },
      "source": [
        "len(text_for_spacy_stripped)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8VSxlEVbWpXH"
      },
      "source": [
        "did = set(text_for_spacy_stripped)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RXoRrckYUowg"
      },
      "source": [
        "len(did)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SVSW2MYb3gDe"
      },
      "source": [
        "doc = nlp(str(did))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZpQUWQb89YV"
      },
      "source": [
        "print(doc.ents)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-HL-Q5Q8Vjx"
      },
      "source": [
        "for np in doc.noun_chunks:\n",
        "    print(np.text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rzbclO5e3hzC"
      },
      "source": [
        "empty_list = []\n",
        "for entity in doc.ents:\n",
        "  print(doc.ents)\n",
        "  empty_list.append(entity.text)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rIYJ2Aru9Mcl"
      },
      "source": [
        "empty_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9GPK1jFKvk7y"
      },
      "source": [
        "for entity in doc.ents:\n",
        "  print(entity.label_, ' | ', entity.text) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S4L24stWvIuk"
      },
      "source": [
        "len(set(empty_list))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZFjtlrQEqpHv"
      },
      "source": [
        "file = open(\"corpus.txt\", \"w\") \n",
        "file.write(str(erst)) \n",
        "file.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F410qj96vvKL"
      },
      "source": [
        "set(empty_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "whmmTCDQQFlC"
      },
      "source": [
        "len(empty_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qt6O1a1ZfUEq"
      },
      "source": [
        "**Word2Vec**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Pbnw_j7fXBP"
      },
      "source": [
        "import multiprocessing\n",
        "from gensim.models import Word2Vec\n",
        "import gensim \n",
        "import gensim.utils\n",
        "from gensim.test.utils import common_texts, get_tmpfile\n",
        "from gensim.models import Word2Vec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3rIriZEhKaM"
      },
      "source": [
        "cores = multiprocessing.cpu_count() # Count the number of cores in a computer\n",
        "cores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fD60QvK1vfNG"
      },
      "source": [
        "import glob\n",
        "file_names = glob.glob(\"/content/clean.txt\")\n",
        "with open(\"/content/clean.txt\", \"r\", encoding = \"utf-8\") as f:\n",
        "      print(f.read())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tot7C2-nvjSC"
      },
      "source": [
        "sport_text_list=[]\n",
        "for file in file_names:\n",
        "    try:\n",
        "        with open(file, \"r\", encoding= \"utf-8\") as f:\n",
        "            sport_text_list.append(f.read())\n",
        "    except:\n",
        "      pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJ2xm3_UtdWu"
      },
      "source": [
        "clean_texts = []\n",
        "for text in sport_text_list:\n",
        "    clean_texts.append(gensim.utils.simple_preprocess(text))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ymalspuVfcZL"
      },
      "source": [
        "sentences = [nltk.word_tokenize(sent) for sent in nltk.sent_tokenize(str(fifth_step))]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NuvhQcfId4xP"
      },
      "source": [
        "model = gensim.models.Word2Vec(#check if works\n",
        "        sentences,\n",
        "        size = 150,\n",
        "        window = 10,\n",
        "        min_count = 5,\n",
        "        workers = 10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11JbB6m6fDE-"
      },
      "source": [
        "vocab = list(model.wv.vocab)\n",
        "vocab"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U8C6ajA7haQa"
      },
      "source": [
        "model = gensim.models.Word2Vec(\n",
        "        clean_texts,\n",
        "        size = 150,\n",
        "        window = 10,\n",
        "        min_count = 1,\n",
        "        workers = 10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wKbAIRkzhlCC"
      },
      "source": [
        "model.train(clean_texts, total_examples=len(third_step), epochs=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZVDgdIIhqDA"
      },
      "source": [
        "model.save(\"word2vec.model\")#save the model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-bmuedQtiA1b"
      },
      "source": [
        "model.wv.save_word2vec_format('model.bin', binary=True)#save the model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1patbmR8dfEw"
      },
      "source": [
        "from collections import defaultdict\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pb8DkqetgAgu"
      },
      "source": [
        "from nltk.tokenize import sent_tokenize, word_tokenize"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rfoqiR8cgGL6"
      },
      "source": [
        "phrases = sent_tokenize(text_from_file)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BmfJdvV7gO6r"
      },
      "source": [
        "print(phrases[:50])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XLjyG490eLlx"
      },
      "source": [
        "line = 'Once upon a time a time this upon a'\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2y2057xLg7FM"
      },
      "source": [
        "line = list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QnVeFJrBf0DQ"
      },
      "source": [
        "bigrams = [b for l in phrases for b in zip(l.split(\" \")[:-1], l.split(\" \")[1:])]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HR7Hod4kkjzP"
      },
      "source": [
        "print(bigrams)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41jZ1c6ueJq7"
      },
      "source": [
        "dic = defaultdict(int)\n",
        "\n",
        "s = list(phrases)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bDyQN5CpiM4N"
      },
      "source": [
        "s"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6IZRUJR2ixqB"
      },
      "source": [
        "len(s)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3jmmotQipU7"
      },
      "source": [
        "s = normalize(s)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWU3WI44dfQt"
      },
      "source": [
        "for i in range(0, len(s)-1):\n",
        "    dic[str(s[i]) + ' ' + str(s[i+1])] += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55pDr9eNhqz6"
      },
      "source": [
        "len(dic)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2WuKWovwiAKN"
      },
      "source": [
        "dic"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vf3jDWA0jY4P"
      },
      "source": [
        "dic"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQG-gtVEOufu"
      },
      "source": [
        "len(model.wv.vocab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKGZdqr_OuR9"
      },
      "source": [
        "path = get_tmpfile(\"/content/word2vec.model\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mR6yCABOuDG"
      },
      "source": [
        "path = get_tmpfile(\"/content/word2vec.model\")\n",
        "\n",
        "model.wv.save_word2vec_format(\"/content/word2vec.txt\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sF7fRCu1Oty-"
      },
      "source": [
        "import gzip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OYRtXwR7jq04"
      },
      "source": [
        "\n",
        "!zip -r /content/file.zip /content/word2vec.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T71Qd6_ojsul"
      },
      "source": [
        "!python -m spacy init-model en ./data/spacy.word2vec.model --vectors-loc word2vec.txt.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KKZoHvFhjvUS"
      },
      "source": [
        "nlp = spacy.load('en', vectors='./data/spacy.word2vec.model/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tGGvMq-SjxeT"
      },
      "source": [
        "doc = nlp(str(third_step))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T8hMBeDDj5UM"
      },
      "source": [
        "for entity in doc.ents:\n",
        "  print(entity.label_, ' | ', entity.text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AnYLRlHrTv_v"
      },
      "source": [
        "#new "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3VBEGqZTxPF"
      },
      "source": [
        "with open(\"/content/USETHIS.txt\", 'rb') as corpus:\n",
        "  text_full = corpus.read()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6eXAfLz7T1Nf"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iq2jGJpqT5AL"
      },
      "source": [
        "text_for_spacy = word_tokenize(str(text_full))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dq7PtiPWT-7g"
      },
      "source": [
        "tag_list = [nltk.pos_tag(w) for w in text_for_spacy]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XlijqiqfUHvH"
      },
      "source": [
        "tag_list = nltk.pos_tag(text_for_spacy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OlhSeSufVSHj"
      },
      "source": [
        "tag_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SkCpzCU4UIez"
      },
      "source": [
        "import matplotlib\n",
        "matplotlib.use('Agg')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pqspE-HfUeZj"
      },
      "source": [
        "NP = \"NP: {(<V\\w+>|<NN\\w?>)+.*<NN\\w?>}\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DrY5pMg9UZRx"
      },
      "source": [
        "chunkr = nltk.RegexpParser(NP)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vsyr8Vw1Ukod"
      },
      "source": [
        "import matplotlib\n",
        "matplotlib.use('Agg')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eea1X1bDUo1O"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.switch_backend('agg')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJAKseozUrtj"
      },
      "source": [
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1LQzKZfvVjj7"
      },
      "source": [
        "import nltk\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "omTZ5rIuVM8H"
      },
      "source": [
        "from nltk import word_tokenize, pos_tag, ne_chunk\n",
        "from nltk import RegexpParser\n",
        "from nltk import Tree\n",
        "import pandas as pd\n",
        "\n",
        "def get_continuous_chunks(text, chunk_func=ne_chunk):\n",
        "    chunked = chunk_func(pos_tag(word_tokenize(text)))\n",
        "    continuous_chunk = []\n",
        "    current_chunk = []\n",
        "\n",
        "    for subtree in chunked:\n",
        "        if type(subtree) == Tree:\n",
        "            current_chunk.append(\" \".join([token for token, pos in subtree.leaves()]))\n",
        "        elif current_chunk:\n",
        "            named_entity = \" \".join(current_chunk)\n",
        "            if named_entity not in continuous_chunk:\n",
        "                continuous_chunk.append(named_entity)\n",
        "                current_chunk = []\n",
        "        else:\n",
        "            continue\n",
        "\n",
        "    return continuous_chunk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zv3ZMbdqVfPW"
      },
      "source": [
        "df = pd.DataFrame({'text':['This is a foo, bar sentence with New York city.', \n",
        "                           'Another bar foo Washington DC thingy with Bruce Wayne.']})\n",
        "\n",
        "df['text'].apply(lambda sent: get_continuous_chunks((sent)))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
